<!doctype html>
<html lang="en">
  <head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE-edge">

    <link rel="icon" href="img/kinect.png">

    <!-- Bootstrap CSS -->
    <link rel="stylesheet" type="text/css" href="css/bootstrap.css">
    <!--fONT AWSOME-->
    <link href="css/fontawesome-all.css" rel="stylesheet">
    <!-- Estilos en CSS -->
    <link rel="stylesheet" type="text/css" href="css/custom.css">

    <title>3D RGB-D Reconstruction</title>
  </head>
  <body>

<!-- IMAGEN DE FONDO PRINCIPAL
    <section class="container-fluid slider d-flex justify-content-center align-items-center">
      <h1 class="display-4">Buena Salud</h1>   
    </section> -->

<!-- HEADER -->

<section class="container text-left pt-4 pb-4">

	<div class="pl-4">
			
		<h2>Dense Scene Reconstruction from RGBD Images</h2>
		<h5>Max W. Portocarrero</h5><br>

		<div class="d-flex justify-content-between">
			<div>
				<p class="font-weight-bold">
					Proyecto Final - Computación Gráfica - Marzo 2018<br> Universidad Católica San Pablo
				</p>     
		    </div>

	    	<div class=" text-right">
	    		<h5>
	        		<a class="" href="https://docs.google.com/presentation/d/1dWHYH7ZswnIV4mFzIAoYth6-2Qc4NLTatDkJcmOL_2s/edit?usp=sharing">Slides</a> &nbsp; | &nbsp; <a href="https://github.com/mxportocarrero/RGB-D_3DReconstruction">Github Package</a> <br>
	    		</h5>
	        	mxportocarrero@gmail.com &nbsp; <i class="fas fa-envelope"></i>
	    	</div>

		</div>
	</div>
	<hr>
	<div class="pl-4">
		<br>
		<p class="font-weight-bold">Profesores:</p>
		<ul>
			<li>Dr. Jorge Poco (PhD. Computer Science - New York University, USA - 2016)</li>
			<li>Dr. Erick Gómez (PhD. Computer Science - Universidae de Sao Paulo, Brasil - 2016)</li>
		</ul>
	</div><br>

	<hr>
</section>

<section class="container">
	<div class="pl-4">
		<p class="font-weight-bold">Artículo Base:</p>
		<p class="text-center font-weight-bold" style="font-size: 28px;">Dense Scene Reconstruction with Points of Interest</p>
		
		<p class="text-center">Quian-Yi Zhou &  Vladlen Koltun<br>Stanford University</p>

	<div class="text-center">
		<img src="img/statues.jpg" alt="Reconstruction" width="800"><br><br>
	</div>

	</div>
	<br>

	<hr>

</section>

<!--VIDEO-->

<section class="container text-center">
	<br>
		<iframe width="640" height="480" frameBorder="0" src="https://www.youtube.com/embed/RqixSH6N3bw"></iframe>
	<br><br>
	<hr>
	
</section>

<!--REPORT-->
<section class="container pt-3">
	<div class="pl-4">	
		<h3>Abstract</h3><br>

		<p class="paper-paragraph text-justify">
			En el siguiente trabajo se presenta un método para resolver el problema "Dense 3D Reconstruction"
			a partir de un Imágenes RGBD. Esto es, generar un mesh (o Superficie) globalmente consistente y detallado de una escena en base
			a un stream continuo (por ejemplo, un video) provenientes de algun sensor especializado, como el Kinect. El usuario puede
			mover el sensor libremente.
		</p>

		<p class="paper-paragraph text-justify">
			Se realizan 3 etapas de procesamiento. Frame Registration, Graph-SLAM Optimization y Volume Integration. Estas etapas además deben solucionar
			el problema de movimiento libre del sensor (6DOF), además de reducir el ruido captado por la poca precision de estos sensores. Más aún,
			se debe tomar en cuenta que este trabajo pretende realizar tal reconstrucción en tiempo real.
		</p>

		<p class="paper-paragraph text-justify">
			Los resultados obtenidos hasta el momento demuestran que es posible realizar una reconstrucción detallada de escenas complejas y medianas
			(aprox. 50 m2) usando estas camáras de consumo masivo.
		</p><br>
	</div>
	<hr>
	<div class="pl-4">
		<h3 class="pt-3">Objetivos del Proyecto</h3><br>

		<p class="paper-paragraph text-justify">
			El propósito del proyecto es estudiar la forma de generar meshes a partir de un stream RGBD provisto por un Sensor Comercial de Profundidad
			como el Kinect. Este problema tiene muchas aplicaciones que van desde la Navegación Autónoma a Programas que usan Realidad Aumentada o Mixta.
			Sin embargo, el impacto de este tipo de información esta limitado por el poco frecuente acceso a este tipo de sensores y al nivel de dificultad
			que conlleva su desarollo. Así nos enfocaremos en:
		</p>

		<ul class="paper-paragraph">
			<li>Usar la Data ó Data Stream provenientes de un sensor RBGD para reconstruir una escena compleja</li>
			<li>Revisar los trabajos recientes y el estado del arte en el área. Aplicar algoritmos y optimizaciones que mejorenel desempeño del paper base</li>
			<li>Desarrollar y mantener un programa que realize la reconstrucción, buscando que sea los mas sencillo y robusto</li>
		</ul><br>
	</div>
	<hr>
	<div class="pl-4">
		<h3 class="pt-3">Trabajo Relacionado</h3><br>

		<p class="paper-paragraph text-justify">
			el trabajo de Reconstrucción es una área de investigación que en los últimos tiempos viene siendo de gran interés; pero sus inicios vienen de hace bastante tiempo.
		</p>
		<p class="paper-paragraph text-justify">
			Los primeros trabajos se atribuyen a los Realizados por (1) y (2) quienes fueron los pioneros en proponer dos variantes del Iterative Closest Point.
			A partir de ese momento el interes en generar modelos 3D fue incremenándose, pero en ese momento no se desarrolló mucho debido a la falta de hardware especializado
			y a lo reciente del área. Es por ello, que el estudio de modelos 3D se dejo a Fotogrametría y la Estereoscopia que era posible aun con una sola cámara.
		</p>
		<p class="paper-paragraph text-justify">
			A pesar de que la Reconstruction Monocular o Stereoscopica se ha desarrollado mucho, tiene limitantes en varios aspectos, como del sensor, algoritmos, calibración, que
			no permiten que estos adquieran escenas muy detalladas o "Up-to-Scale". Lo contrario pasa si usamos además un sensor de profundidad. Los sensores han ido evolucionando,asi mismo la tecnología que los opera, lo que permite una mayor precisión y rango de uso, lo suficiente como para obtener pequeños detalles en una escena compleja.
		</p>
		<p class="paper-paragraph text-justify">
			Actualmente, varios autores han desarrollado distintos enfoques con objetivos diferentes. Los enfoques pueden llegar desde lo probabilista a lo aproximado o exacto y en
			cuanto a los objetivos normalmente se persigue obtener mayor precisión o mayor rapidez. Sin embargo, es dificil obtener ambos en estos problemas.
		</p>
		<p class="paper-paragraph text-justify">
			El presente trabajo se basa en el paper de Zhou. Más, es importante anotar que se reviso el Estado del Arte del Área y se busco aquellas optimizaciones que hiceran
			mas sencillo y eficiente al algoritmo. Esto teniendo como meta relizar un programa lo más preciso y rápido posible.
		</p>
		<br>
	</div>
	<hr>
	<div class="pl-4">
		<h3 class="pt-3">Limitaciones</h3><br>

		<ul class="paper-paragraph">
			<li>Mantener el Voxel Grid usado para la TSDF consume demasiada memoria, lo que generalmente puede aliviarse con alguna estructura de subdivision espacial, como
			un OcTree</li>
			<li>Cualquier modo de integración de las imágenes, sea frame-to-frame o frame-to-model(más robusto para escenas grandes) esta limitado por la poca
				precision que tienen inherentemente los sensores. Solo esperar el desarrollo de mejores sensores con mejor precision, mayor alcance y que puedan trabajar
				en Outdoor Scenes.</li>
			<li>Debido a lo anterior, se espera una acumulación de Registration Errors en toda la trayectoria de la cámara. Esto se pueden minimizar de distintas formas,
				la más popular es resolviendo el problema de Graph-SLAM</li>
		</ul>

		<br>
	</div>
	<hr>
	<div class="pl-4">
		<h3 class="pt-3">Overview</h3><br>

		<p class="paper-paragraph text-justify">
			El trabajo se centro en realizar tres tareas específicas:			
		</p>
		<ul class="paper-paragraph">
			<li>Frame Registration</li>
			<li>Graph-SLAM Optimization</li>
			<li>Volume Integration</li>
		</ul>
		<p class="paper-paragraph text-justify">
			Cada una de estas etapas pueden realizarse de diferentes formas y teniendo distintos enfoques, a continuación
			damo una breve explicación de cada una de ellas:
		</p>

		<h5 class="pt-3">Frame Registration</h5>

		<p class="paper-paragraph text-justify">
			En esta etapa procesamos la Raw Data proveniente del sensor. Esta data debe pasar por un pre-procesamiento
			de calibración y sincronización. Esto es para asegurarnos que tanto la imagen RGB y la imagen de Depth se registren
			mutuamente. Es decir, que la correspondencia en pixeles sea 1 a 1. Observemos que la mayoría de Data de Prueba generalmente
			cuenta con estas condiciones.
		</p>
		<p class="paper-paragraph text-justify">
			El siguiente paso consiste en extraer la información geométrica de las imagenes (Nubes de Puntos, Colores, Normales). Ello generalmente, se
			realiza con la matriz intrínseca de la cámara.
		</p>

		<p class="paper-paragraph text-justify">
			Una vez obtenidos las nubes de Puntos es necesario Alinear estas nubes. Esto es, el proceso de Registración. Para ello, se hace uso de alguna variante de ICP.
			El ICP tiene como objetivo calcular la matriz de transformación(6DOF) que minimize alguna metrica de error sobre ambas nubes de puntos.
		</p>

		<p class="paper-paragraph text-justify">
			Existen varias formas de realizar el ICP, sin embargo, en el presente trabajo se aprovecharon las images RGB, de las cuales se extrajeron ORB features.
			Estos se usaron para asegurar correspondencias entre las nubes de puntos y poder aplicar Point-to-Point ICP. Esta parte se optimizo haciendo uso del algoritmo RANSAC.
			Este último no solo permite una convergencia mas ŕapida del ICP, sino también elimina outliers o ruido que se presente en las nubes.
		</p>

		<h5 class="pt-3">Graph-SLAM Optimization</h5>

		<p class="paper-paragraph text-justify">
			Con el conjunto de Matrices de Transformación para frames consecutivos es posible calcular o al menos dibujar un alineamiento bueno de los puntos, pero no
			preciso. La consecución de la trayectoria en base de un frame anterior, trae como consecuencia acumulación de Drift, o error acumulado, que se incrementa mientras
			mas larga sea la trayectoria.
		</p>
		<p class="paper-paragraph text-justify">
			Para minimizar este problema y asegurarnos de que nuestra reconstrucción sea consistente, se realiza un proceso de optimización que busca repartir el error acumulado por
			Drift entre toda o en partes de la trayectoria. Para Ello construye un grafo, conocido como Graph-SLAM, donde cada nodo es un frame y cada arista es una Matrix de Transformación. Una de las tareas mas importantes en esta etapa es el Reconocimiento de Loop Closures, que es posible tambien a través de Image Features. La minimización del error se realiza construyendo una adecuada función de error(minimizable) y que pueda ser resuelta por algún Método Numérico como el de Gaus-Newton. Es posible utilizar solvers como el g2o.
		</p>

		<h5 class="pt-3">Volume Integration</h5>

		<p class="paper-paragraph text-justify">
			Ya teniendo las transformaciones optimizadas. La siguiente etapa consiste en la integración de los datos de cada frame en una sola Escena Global. Para ello se han realizado
			bastante cantidad de modelos. Sin embargo el más efectivo es construir una Truncated Signed Distance Function(TSDF) sobre un volume global. Esta TSDF, irá actualizando sus valores en cada frame, para al final contener en cada voxel, las medidas aproximadas a la superficie (del objeto) más cercanas. Este proceso se conoce como Integración
		</p>
		<p class="paper-paragraph text-justify">
			Una vez se cuenta con una TSDF integrada por todos los frames del stream RGBD. Es necesario visualizar los resultados. Esto último se consigue haciendo uso de Ray Casting o algún método de poligonalización como el algoritmo de Marching Cubes. Adicionalmente, TSDF se construbyó sobre un OcTree que permite una evaluación mucho más rapida de los voxels.
		</p>
		<br>

	</div>
	<hr>
	<div class="pl-4">
		<h3 class="pt-3">Resultados y Experimentos</h3><br>

		<p class="paper-paragraph text-justify">
			Como base para nuestros experimentos se tomo la Data propuesta por Sturm y los streams de Zhou, ambos ubicados en sus páginas personales. Refiérase a sus papers para mas detalles
		</p>
		<p class="paper-paragraph text-justify">
			Debido a que aún no se llegado a completar el proyecto nuestra única métrica disponible por el momento es el tiempo de procesamiento.
		</p>
		<p class="paper-paragraph text-justify">
			En el Paper Base, al ser un algoritmo offline, tiene tiempos de procesamiento muy largos, siendo el mínimo de 1h 20 min en ~15m2. En nuestro avance vemos que al emplear el ICP combinado con ORB y RANSAC el tiempo de registración es bajísimo siendo a lo más de algunos segundos.
		</p><br>
	</div>

	<div class="text-center">
		<img src="img/statue.jpeg" alt="Reconstruction" width="400">
	</div>
	<br><br>

	<hr>
	<div class="pl-4">
		<h3 class="pt-3">Discusión</h3><br>
		<p class="paper-paragraph text-justify">
			Los avances hasta el momento indican una reducción considerable en el tiempo de procesamiento. Tal es asi, que es posible pensar, que el algoritmo puedo correr
			en Tiempo Real. Sin embargo, no se puede asegurar nada, debido a que no se conoce como afectará nuevas optimizaciones(mas procesamiento) al algoritmo.
		</p>

		<p class="paper-paragraph text-justify">
			Zhou, como se describe en el paper Base, realiza una serie de optimizaciones y calculos para asegurar la precision. Sin embargo, trabajos mas modernos. Son capaces de lograr mucha mas precision que Zhoud, aún corriendo en Tiempo-Real. Por ello, se debería cuestionar cual es el grado o número de optimizaciones necesarias que nos lleven
			a tiempos aceptables de procesamiento que no sacrifiquen precision o consistencia.
		</p>
		<br>
	</div>
	<hr>
	<div class="pl-4">
		<h3>Referencias</h3><br>

		<ol>
			<li>Besl & McKay,1992,A Method for Registration of 3-D Shapes</li>
			<li>Chen & Medioni, 1992, Object Modelling by Registration of Multiple Range Images</li>
			<li>Curless & Levoy, 1996, A Volumetric Method for Building Complex from Range Images</li>
			<li>Dai et. al., 2017, BundleFusion: Real-time Globally Consistent 3D Reconstruction using On-the-fly Surface Re-integration</li>
			<li>Eggert,1997, Estimating 3-D rigid body Transformations: a comparison of four major algorithms</li>
			<li>Endres et. al., 2012, An Evaluation of the RGB-D SLAM System</li>
			<li>Endres et. al., 2014, 3-D Mapping with and RGB-D Camera</li>
			<li>Fischler, 1981, Random Sample Concensus: A Paradigm for Model Fitting with Applications to Image Analisys and Automated Cartography</li>
			<li>Furhmann,2011, Fusion of Depth Maps with Multiple Scales (*)</li>
			<li>Hornung et. al., 2013, An Efficient Probabilistic 3D Mapping Framework Based on Octrees</li>
			<li>Knapitsch,2017,Tanks and Temples: Benchmarking Large-Scale Scene Reconstruction</li>
			<li>Lorensen & Cline,1987, Marching Cubes: A High Resolution 3D Surface construction algorithm</li>
			<li>Maie et. al., 2017, Efficient Online Surface Correction for Real-time Large-Scale 3D Reconstruction</li>
			<li>Newcombe et. al., 2011, Kinect-Fusion: Real-Time Dense Surface Mapping and Tracking</li>
			<li>Park, Zhou & Koltun, 2017, Colored Point Cloud Registration</li>
			<li>Parker et. al., 1998, Interactive Ray Tracing for Isosurface Rendering</li>
			<li>Rublee et. al., 2011, ORB: an efficient alternative to SIFT or SURF</li>
			<li>Rusinkiewicz, 2001, Efficient Variants of the ICP ALgorithm</li>
			<li>Segal et. al., 2009, Generalized-ICP</li>
			<li>SteinBrucker Sturm & Cremers, 2011, Real-time Visual Odometry from Dense RGB-D Images</li>
			<li>Sturm,2012, A benchmark for the evaluation of multi.view stereo reconstruction algorithms</li>
			<li>Turk & Levoy,1994, Zippered Polygon Meshes from Range Images</li>
			<li>Umeyama,1991, Least-Squares Estimation of Transformation Parameters Between Two Points Patterns</li>
			<li>Whelan et. al., 2016, ElasticFusion: Real-Time Dense SLAM and Light Source Estimation</li>
			<li>Zhou, 2013, Dense Scene Reconstruction from Points of Interest (**)</li>
			<li>Zhou, 2014, Color Map Optimization for 3D Reconstruction with Consumer Depth Cameras</li>
			<li>Zhou, 2014, Simultaneous Localization and Calibration: Self-Calibration of Consumer Depth Cameras</li>
			<li>Zhou, 2015, Robust Reconstruction of Indoor Scenes</li>
			<li>Zhou, 2016, Fast Global Registration</li>
			<li>Zhou, 2018, Open3D: A Modern Library for 3D Data Processing</li>
		</ol>
		* Revisar estos papers<br>
		** Paper base del Proyecto<br><br>
	</div>
	<hr>
</section>
	


</main>

<!-- footer -->
  <footer>
    <section class="container my-3 py-3">
      <div class="row">
      <div class="col">
      Simple 3D Reconstruction (S3DR) &copy; 2018. Todos los derechos reservados.
      </div>
      <div class="col text-right" style="font-size: 12px;"> Diseño y Desarrollo por <a href="#">M. Portocarrero</a></div>
      </div>
    </section>
  </footer>
<!-- footer -->



    <!-- Optional JavaScript -->
    <!-- jQuery first, then Popper.js, then Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
    <script src="js/bootstrap.min.js"></script>


  </body>


</html>